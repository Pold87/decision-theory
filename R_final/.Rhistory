points <- apply(raw_ratings, 1, function(x) x - real_stuff$nPVI)
agg.points <- apply(points, 2, function(x) sum(x == 0))
df.agg.points <- data.frame(agg.points)
# Determine difficulty of a song
# TODO: Split into metrical / non-metrical
agg.difficulty.song <- apply(points, 1, function(x) {sum(x == 0)})
df.agg.difficulty.song <- as.data.frame(agg.difficulty.song)
df.agg.difficulty.song <- cbind(song = rownames(df.agg.difficulty.song), df.agg.difficulty.song)
rownames(df.agg.difficulty.song) <- NULL
names(df.agg.difficulty.song)[names(df.agg.difficulty.song) == 'agg.difficulty.song'] <- 'correct'
df.agg.difficulty.song$song <- as.character(df.agg.difficulty.song$song)
df.agg.difficulty.song$song <- factor(df.agg.difficulty.song$song,
levels = unique(df.agg.difficulty.song$song))
p.difficulty <- ggplot(df.agg.difficulty.song, aes(x = song,
y = correct))
p.difficulty <- p.difficulty + geom_bar(stat = 'identity')
# Find IDs of same presentations (i.e. first and second presentation of a song)
agg.real_stuff <- aggregate(id ~ original, real_stuff, identity)
# Extract first ratings
rating1 <- points[agg.real_stuff$id[, 1], ]
# Extract second ratings
rating2 <- points[agg.real_stuff$id[, 2], ]
# Consistency (i.e. the deviation from rating1 to rating2)
# Accuracy metric
acc.metric <- apply(rating1, 2, function(x) {sum(x == 0)})
# Accuracy non-metric
acc.nonmetric <- apply(rating2, 2, function(x) {sum(x == 0)})
# WRONG !! (It shows the percentage of correct answers per person / song)
rating.consistency <- rating2 - rating1
rating.consistency.by.song <- adply(points, 1, function(x) {sum(x == 0)})
rating.consistency.by.song$V1 <- rating.consistency.by.song$V1 / ncol(points)
rating.consistency.by.person <- adply(points, 2, function(x) {sum(x == 0)})
rating.consistency.by.person$V1 <- rating.consistency.by.person$V1 / nrow(points)
rating.consistency.real.by.song <- adply(rating.consistency, 1, function(x) {sum(x == 0)})
rating.consistency.real.by.song$V1 <- rating.consistency.real.by.song$V1 / ncol(rating.consistency)
rating.consistency.real.by.person <- adply(rating.consistency, 2, function(x) {sum(x == 0)})
rating.consistency.real.by.person$V1 <- rating.consistency.real.by.person$V1 / nrow(rating.consistency)
# Plot histogram of correct answers
p <- ggplot(df.agg.points, aes(x = agg.points, fill = ..x..))
p <- p + geom_histogram(binwidth = 4)
p <- p + ggtitle("Histogram of correct answers")
p <- p + geom_vline(xintercept=12, linetype = "longdash")
p <- p + geom_vline(xintercept=36, linetype = "longdash")
p <- p + scale_x_continuous(breaks=seq(0, 36, 4))
p <- p + coord_cartesian(xlim=c(0, 36))
rep.row<-function(x,n){
matrix(rep(x,each=n),nrow=n)
}
# Build a confusion matrix
df.true_values <- rep.row(real_stuff$nPVI, 13)
m.true_values <- data.matrix(df.true_values)
m.raw_ratings <- data.matrix(raw_ratings)
xtab <- table(m.raw_ratings, m.true_values)
confusionMatrix(xtab)
confusionMatrix(m.raw_ratings, m.true_values)
long.raw_ratings <- melt(raw_ratings.matrix[-1, ])
long.raw_ratings$true_nPVI <- rep(real_stuff$nPVI, 13)
long.raw_ratings$metrical <- rep(real_stuff$metrical, 13)
colnames(long.raw_ratings)[1:3] <- c("song", "participant", "guessed_nPVI")
long.raw_ratings$difference <- long.raw_ratings$true_nPVI - long.raw_ratings$guessed_nPVI
long.raw_ratings$correct <- sapply(long.raw_ratings$difference, function(x) (x == 0) * 1)
long.points <- melt(points)
long.points.trueorfalse <- long.points
long.points.trueorfalse[3] <- lapply(long.points[3], function(x) (x == 0) * 1)
# Convert long format to wide format
data.wide <- dcast(long.raw_ratings,
participant ~ metrical + true_nPVI,
value.var = "correct",
fun.aggregate = mean)
# Use an aggregated long version of the wide data format (shows accuracy per condition)
data.to.long.again <- melt(data.wide)
# Do ANOVA
# H0: Mean accuracy is the same for all conditions
attach(data.to.long.again)
aov(value ~ variable)
kruskal.test(value ~ variable)
interaction.plot(metrical, true_nPVI, correct)
attach(long.raw_ratings)
require(ggplot2)
require(plyr)
require(caret)
# Read in CSV files
raw_ratings <- read.csv("raw_ratings.csv", check.names = FALSE)
raw_ratings.matrix <- apply(raw_ratings, 1, function(x) x)
rownames(raw_ratings) <- raw_ratings[,1]
raw_ratings <- raw_ratings[, -1]
real_stuff <- read.csv("/home/pold/npvi/versions/aubio/overview.csv")
# Determine correct answers
points <- apply(raw_ratings, 1, function(x) x - real_stuff$nPVI)
agg.points <- apply(points, 2, function(x) sum(x == 0))
df.agg.points <- data.frame(agg.points)
# Determine difficulty of a song
# TODO: Split into metrical / non-metrical
agg.difficulty.song <- apply(points, 1, function(x) {sum(x == 0)})
df.agg.difficulty.song <- as.data.frame(agg.difficulty.song)
df.agg.difficulty.song <- cbind(song = rownames(df.agg.difficulty.song), df.agg.difficulty.song)
rownames(df.agg.difficulty.song) <- NULL
names(df.agg.difficulty.song)[names(df.agg.difficulty.song) == 'agg.difficulty.song'] <- 'correct'
df.agg.difficulty.song$song <- as.character(df.agg.difficulty.song$song)
df.agg.difficulty.song$song <- factor(df.agg.difficulty.song$song,
levels = unique(df.agg.difficulty.song$song))
p.difficulty <- ggplot(df.agg.difficulty.song, aes(x = song,
y = correct))
p.difficulty <- p.difficulty + geom_bar(stat = 'identity')
# Find IDs of same presentations (i.e. first and second presentation of a song)
agg.real_stuff <- aggregate(id ~ original, real_stuff, identity)
# Extract first ratings
rating1 <- points[agg.real_stuff$id[, 1], ]
# Extract second ratings
rating2 <- points[agg.real_stuff$id[, 2], ]
# Consistency (i.e. the deviation from rating1 to rating2)
# Accuracy metric
acc.metric <- apply(rating1, 2, function(x) {sum(x == 0)})
# Accuracy non-metric
acc.nonmetric <- apply(rating2, 2, function(x) {sum(x == 0)})
# WRONG !! (It shows the percentage of correct answers per person / song)
rating.consistency <- rating2 - rating1
rating.consistency.by.song <- adply(points, 1, function(x) {sum(x == 0)})
rating.consistency.by.song$V1 <- rating.consistency.by.song$V1 / ncol(points)
rating.consistency.by.person <- adply(points, 2, function(x) {sum(x == 0)})
rating.consistency.by.person$V1 <- rating.consistency.by.person$V1 / nrow(points)
rating.consistency.real.by.song <- adply(rating.consistency, 1, function(x) {sum(x == 0)})
rating.consistency.real.by.song$V1 <- rating.consistency.real.by.song$V1 / ncol(rating.consistency)
rating.consistency.real.by.person <- adply(rating.consistency, 2, function(x) {sum(x == 0)})
rating.consistency.real.by.person$V1 <- rating.consistency.real.by.person$V1 / nrow(rating.consistency)
# Plot histogram of correct answers
p <- ggplot(df.agg.points, aes(x = agg.points, fill = ..x..))
p <- p + geom_histogram(binwidth = 4)
p <- p + ggtitle("Histogram of correct answers")
p <- p + geom_vline(xintercept=12, linetype = "longdash")
p <- p + geom_vline(xintercept=36, linetype = "longdash")
p <- p + scale_x_continuous(breaks=seq(0, 36, 4))
p <- p + coord_cartesian(xlim=c(0, 36))
rep.row<-function(x,n){
matrix(rep(x,each=n),nrow=n)
}
# Build a confusion matrix
df.true_values <- rep.row(real_stuff$nPVI, 13)
m.true_values <- data.matrix(df.true_values)
m.raw_ratings <- data.matrix(raw_ratings)
xtab <- table(m.raw_ratings, m.true_values)
confusionMatrix(xtab)
confusionMatrix(m.raw_ratings, m.true_values)
long.raw_ratings <- melt(raw_ratings.matrix[-1, ])
long.raw_ratings$true_nPVI <- rep(real_stuff$nPVI, 13)
long.raw_ratings$metrical <- rep(real_stuff$metrical, 13)
colnames(long.raw_ratings)[1:3] <- c("song", "participant", "guessed_nPVI")
long.raw_ratings$difference <- long.raw_ratings$true_nPVI - long.raw_ratings$guessed_nPVI
long.raw_ratings$correct <- sapply(long.raw_ratings$difference, function(x) (x == 0) * 1)
long.points <- melt(points)
long.points.trueorfalse <- long.points
long.points.trueorfalse[3] <- lapply(long.points[3], function(x) (x == 0) * 1)
# Convert long format to wide format
data.wide <- dcast(long.raw_ratings,
participant ~ metrical + true_nPVI,
value.var = "correct",
fun.aggregate = mean)
# Use an aggregated long version of the wide data format (shows accuracy per condition)
data.to.long.again <- melt(data.wide)
# Do ANOVA
# H0: Mean accuracy is the same for all conditions
attach(data.to.long.again)
aov(value ~ variable)
kruskal.test(value ~ variable)
attach(long.raw_ratings)
interaction.plot(metrical, true_nPVI, correct)
long.raw_ratings <- melt(raw_ratings.matrix[-1, ])
long.raw_ratings <- melt(raw_ratings.matrix[-1, ])
long.raw_ratings$true_nPVI <- rep(real_stuff$nPVI, 13)
long.raw_ratings$metrical <- rep(real_stuff$metrical, 13)
colnames(long.raw_ratings)[1:3] <- c("song", "participant", "guessed_nPVI")
long.raw_ratings$difference <- long.raw_ratings$true_nPVI - long.raw_ratings$guessed_nPVI
long.raw_ratings$correct <- sapply(long.raw_ratings$difference, function(x) (x == 0) * 1)
require(ggplot2)
require(plyr)
require(caret)
install.packages(c("caret", "ggplot2", "plyr"))
require(ggplot2)
require(plyr)
require(caret)
# Read in CSV files
raw_ratings <- read.csv("raw_ratings.csv", check.names = FALSE)
raw_ratings.matrix <- apply(raw_ratings, 1, function(x) x)
rownames(raw_ratings) <- raw_ratings[,1]
raw_ratings <- raw_ratings[, -1]
real_stuff <- read.csv("/home/pold/npvi/versions/aubio/overview.csv")
overview <- "C:\Users\User\PycharmProjects\music-cognition-dropbox\versions\aubio\overview.csv"
# Read in CSV files
raw_ratings <- read.csv("raw_ratings.csv", check.names = FALSE)
raw_ratings.matrix <- apply(raw_ratings, 1, function(x) x)
rownames(raw_ratings) <- raw_ratings[,1]
raw_ratings <- raw_ratings[, -1]
real_stuff <- read.csv(overview)
overview <- "C:\Users\User\PycharmProjects\music-cognition-dropbox\versions\aubio\overview.csv"
overview <- "C:\\Users\User\PycharmProjects\music-cognition-dropbox\versions\aubio\overview.csv"
"overview.csv")
overview <- file.path("C:", "Users", "User", "PycharmProjects",
"music-cognition-dropbox",
"versions",
"aubio",
"overview.csv")
# Read in CSV files
raw_ratings <- read.csv("raw_ratings.csv", check.names = FALSE)
raw_ratings.matrix <- apply(raw_ratings, 1, function(x) x)
rownames(raw_ratings) <- raw_ratings[,1]
raw_ratings <- raw_ratings[, -1]
real_stuff <- read.csv(overview)
source('~/.active-rstudio-document')
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
install.packages("e1071")
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
View(long.raw_ratings)
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
data.wide.spss <- dcast(long.raw_ratings,
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
data.wide.spss <- dcast(long.raw_ratings,
)
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R', echo=TRUE)
data.wide.spss
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
~ dta.wid
data.wide.spss
source('~/.active-rstudio-document', echo=TRUE)
data.wide.spss
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
write.csv(data.wide.spss, "dataforspss.csv")
source('~/.active-rstudio-document', echo=TRUE)
senna <- long.raw_ratings[long.raw_ratings$participant == 15, ]]
senna <- long.raw_ratings[long.raw_ratings$participant == 15, ]
seanna
senna
long.raw_ratings
senna <- long.raw_ratings[long.raw_ratings$participant == 13, ]
senna
senna_file <- file.path("C:", "Users", "User", "PycharmProjects",
source('~/.active-rstudio-document', echo=TRUE)
)
"senna.csv")
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
data.wide.senna
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
senna
source('~/.active-rstudio-document', echo=TRUE)
data.wide.tom
source('~/.active-rstudio-document', echo=TRUE)
data.wide.tom
source('~/.active-rstudio-document', echo=TRUE)
data.wide.senna
write.csv(data.wide.senna, "data-wide-senna.csv")
write.csv(data.wide.tom, "data-wide-senna.csv")
source('~/.active-rstudio-document', echo=TRUE)
source('C:/Users/User/PycharmProjects/music-cognition-dropbox/evaluation/evaluation.R')
clear
clc
source('~/decision-theory-all/algo/R_final/copula.R', echo=TRUE)
install.packages(c("copula", "VineCopula", "matlab", "psych"))
install.packages(c("caret", "digest", "e1071", "foreach", "ggplot2", "gtable", "gtools", "iterators", "lme4", "munsell", "pbkrtest", "plyr", "quantreg", "Rcpp", "RcppEigen", "scales", "SparseM"))
install.packages("D:/Downloads/VineCopula_2.0.1.zip", repos = NULL, type = "win.binary")
remove.packages('VineCopula')
install.packages("D:/Downloads/VineCopula_2.0.1.zip", repos = NULL, type = "win.binary")
require(copula)
require(VineCopula)
require(copula)
require(VineCopula)
install.packages(c("copula", "VineCopula", "matlab", "psych", "lattice"))
require(copula)
require(VineCopula)
require(matlab)
require(psych)
require(lattice)
source('extract_rgb.R')
setwd("~/decision-theory-all/algo/R_final")
read.from.csv <- TRUE
## Read data (R, G, B, x, y)
if (read.from.csv) {
all.vals <- read.csv('data_stripes.csv')
} else {
all.vals <- extract.rgb('rainbow2', write2csv = T)
}
# Create training and test indices
train.idx <- 1:500
test.idx <- 501:1000
## Split into training and test data
X.train <- all.vals[train.idx, ]
X.test <- all.vals[test.idx, ]
Y.train <- as.matrix(all.vals[train.idx, ])
Y.test <- as.matrix(all.vals[test.idx, ])
## Rank data (i.e., convert to CDF)
X.train.pseudo <- pobs(as.matrix(X.train), ties.method = 'random')
X.test.pseudo <- pobs(as.matrix(X.test), ties.method = 'random')
## Convert to a copuladata object  (as used by the Vine Copula package)
X.train.cop <- as.copuladata(X.train.pseudo)
X.test.cop <- as.copuladata(X.test.pseudo)
rvm <- RVineStructureSelect(X.train.cop, selectioncrit="logLik",
indeptest=TRUE, level=0.05,
familyset=NA)
rvm
pairs.copuladata(X.train.cop)
construct.img <- function(val, x.max=640, y.max=480) {
dim(val) <- c(x.max, y.max)
return(val)
}
## TODO: print coordinates of maximum value
x.max = 640
y.max = 480
N.test = 10
xy <- expand.grid(1:x.max, 1:y.max)
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
for (i in 1:N.test) {
RGB <- X.test.cop[i, c("R", "G", "B")]
cols <- rep(RGB, nrow(xy))
mesh <- cbind(cols, xy.df)
colnames(mesh) <- c("x", "y", "R", "G", "B")
#df <- construct.conditional.df(RGB)
val <- RVinePDF(mesh, rvm)
img <- construct.img(val)
}
x.max = 320
y.max = 240
N.test = 10
xy <- expand.grid(1:x.max, 1:y.max)
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
for (i in 1:N.test) {
RGB <- X.test.cop[i, c("R", "G", "B")]
cols <- rep(RGB, nrow(xy))
mesh <- cbind(cols, xy.df)
colnames(mesh) <- c("x", "y", "R", "G", "B")
#df <- construct.conditional.df(RGB)
val <- RVinePDF(mesh, rvm)
img <- construct.img(val)
}
x.max = 160
y.max = 120
N.test = 10
xy <- expand.grid(1:x.max, 1:y.max)
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
for (i in 1:N.test) {
RGB <- X.test.cop[i, c("R", "G", "B")]
cols <- rep(RGB, nrow(xy))
mesh <- cbind(cols, xy.df)
colnames(mesh) <- c("x", "y", "R", "G", "B")
#df <- construct.conditional.df(RGB)
val <- RVinePDF(mesh, rvm)
img <- construct.img(val)
}
nrow(xy)
nrow(xy)
x.max = 160
y.max = 120
N.test = 10
xy <- expand.grid(1:x.max, 1:y.max)
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
for (i in 1:N.test) {
RGB <- X.test.cop[i, c("R", "G", "B")]
cols <- rep(RGB, nrow(xy))
mesh <- cbind(cols, xy.df)
colnames(mesh) <- c("x", "y", "R", "G", "B")
#df <- construct.conditional.df(RGB)
val <- RVinePDF(mesh, rvm)
img <- construct.img(val)
}
img
for (i in 1:N.test) {
RGB <- X.test.cop[i, c("R", "G", "B")]
cols <- rep(RGB, nrow(xy))
mesh <- cbind(cols, xy.df)
#colnames(mesh) <- c("x", "y", "R", "G", "B")
#df <- construct.conditional.df(RGB)
#val <- RVinePDF(mesh, rvm)
#img <- construct.img(val)
}
head(mesh)
head(cols)
RGB
rep(RGB, 2)
typeof(RGB)
str(RGB)
a = c(1, 2)
typeof(a)
a
unlist(RGB)
typeof(unlist(RGB))
for (i in 1:N.test) {
RGB <- X.test.cop[i, c("R", "G", "B")]
cols <- rep(unlist(RGB), nrow(xy))
mesh <- cbind(cols, xy.df)
#colnames(mesh) <- c("x", "y", "R", "G", "B")
#df <- construct.conditional.df(RGB)
#val <- RVinePDF(mesh, rvm)
#img <- construct.img(val)
}
mesh
head(mesh)
nrow(cols)
cols <- data.frame(rep(unlist(RGB), nrow(xy)))
cols
head(cols)
install.packages("mefa")
require(mefa)
require(mefa)
rep(RGB, 2)
cols <- data.frame(rep(unlist(RGB), nrow(xy)))
head(cols)
rep(unlist(RGB), 10)
x.max = 160
y.max = 120
N.test = 10
xy <- expand.grid(1:x.max, 1:y.max)
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
for (i in 1:N.test) {
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
R = X.test.cop[i, "R"]
G = X.test.cop[i, "G"]
B = X.test.cop[i, "B"]
R.all <- rep(R, nrow(xy))
G.all <- rep(G, nrow(xy))
B.all <- rep(B, nrow(xy))
xy.df$R <- R.all
xy.df$G <- G.all
xy.df$B <- B.all
colnames(xy.df) <- c("x", "y", "R", "G", "B")
xy.df <- xy.df[c("R", "G", "B", "x", "y")]
mesh <- cbind(cols, xy.df)
val <- RVinePDF(mesh, rvm)
img <- construct.img(val)
}
str(mesh)
rvm
str(rvm)
x.max = 160
y.max = 120
N.test = 10
xy <- expand.grid(1:x.max, 1:y.max)
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
str(xy.df)
x.max = 160
y.max = 120
N.test = 10
xy <- expand.grid(1:x.max, 1:y.max)
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
for (i in 1:N.test) {
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
R = X.test.cop[i, "R"]
G = X.test.cop[i, "G"]
B = X.test.cop[i, "B"]
G.all <- rep(G, nrow(xy))
xy.df$R <- R.all
xy.df$B <- B.all
xy.df <- xy.df[c("R", "G", "B", "x", "y")]
img <- construct.img(val)
}
B.all <- rep(B, nrow(xy))
val <- RVinePDF(xy.df, rvm)
colnames(xy.df) <- c("x", "y", "R", "G", "B")
xy.df$G <- G.all
R.all <- rep(R, nrow(xy))
x.max = 160
y.max = 120
N.test = 10
xy <- expand.grid(1:x.max, 1:y.max)
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
for (i in 1:N.test) {
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
R = X.test.cop[i, "R"]
G = X.test.cop[i, "G"]
B = X.test.cop[i, "B"]
R.all <- rep(R, nrow(xy))
G.all <- rep(G, nrow(xy))
B.all <- rep(B, nrow(xy))
xy.df$R <- R.all
xy.df$G <- G.all
xy.df$B <- B.all
xy.df <- xy.df[c("R", "G", "B", "x", "y")]
img <- construct.img(val)
val <- RVinePDF(xy.df, rvm)
}
colnames(xy.df) <- c("x", "y", "R", "G", "B")
head(xy.df)
xy.df <- xy.df[c("R", "G", "B", "x", "y")]
val <- RVinePDF(xy.df, rvm)
img <- construct.img(val)
x.max = 640
y.max = 480
N.test = 10
xy <- expand.grid(1:x.max, 1:y.max)
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
for (i in 1:N.test) {
xy.df <- data.frame(pobs(as.matrix(xy), ties.method = 'random'))
R = X.test.cop[i, "R"]
G = X.test.cop[i, "G"]
B = X.test.cop[i, "B"]
R.all <- rep(R, nrow(xy))
G.all <- rep(G, nrow(xy))
B.all <- rep(B, nrow(xy))
xy.df$R <- R.all
xy.df$G <- G.all
xy.df$B <- B.all
colnames(xy.df) <- c("x", "y", "R", "G", "B")
xy.df <- xy.df[c("R", "G", "B", "x", "y")]
val <- RVinePDF(xy.df, rvm)
img <- construct.img(val)
}
imagesc(img)
